---
title: "Breaking Mode Collapse: How Verbalized Sampling Restores LLM Creativity"
description: "Ask for a distribution, not a single answer."
pubDatetime: 2025-10-03T10:00:00Z
tags: ["AI", "LLM", "Research", "Diversity"]
author: "VS Research Team"
featured: true
---

import VSPlayground from '@/components/interactives/VSPlayground.tsx';
import { Figure, Table, Equation, Sidenote } from '@/components/academic';
import QuickStart from '@/components/sections/QuickStart.tsx';
import QualitativeExamples from '@/components/sections/QualitativeExamples.tsx';
import PromptCard from '@/components/ui/PromptCard.tsx';
import CopyButton from '@/components/ui/CopyButton.tsx';
import CodeBlock from '@/components/ui/CodeBlock.tsx';

// New components for Day 4
import OpeningHook from '@/components/sections/OpeningHook.tsx';
import AhaMoment from '@/components/sections/AhaMoment.tsx';
import FAQ from '@/components/sections/FAQ.tsx';
import ScalingTrendVisualization from '@/components/evidence/ScalingTrendVisualization.tsx';
import SyntheticDataPerformance from '@/components/evidence/SyntheticDataPerformance.tsx';
import VSVariantsComparison from '@/components/sections/VSVariantsComparison.tsx';
import { TrainingFreeBadge, TrainingFreeCallout, VSBeatsFineTuningCallout } from '@/components/ui/TrainingFreeBadge.tsx';

# Breaking Mode Collapse: How Verbalized Sampling Restores LLM Creativity

**Ask for a distribution, not a single answer.** <TrainingFreeBadge client:load variant="inline" />

<!-- Opening hook with relatable examples -->
<OpeningHook client:load />

## TL;DR

- **Problem**: Post-training alignment causes mode collapse<Sidenote number={1}>Mode collapse reduces output diversity, making models favor stereotypical responses.</Sidenote>, reducing LLM creativity by 60-80% [@zhang2025vs]
- **Cause**: We identify *typicality bias* (Œµ>0) in preference data as a fundamental driver
- **Solution**: Verbalized Sampling (VS) - prompt for a distribution with probabilities to recover diversity
- **Key Finding**: <span className="font-bold text-blue-600">Larger models benefit 1.5-2√ó more from VS</span>

<QuickStart client:load />

<!-- Training-free emphasis -->
<TrainingFreeCallout client:load />

<!-- The core insight explained clearly -->
<AhaMoment client:load />

## Interactive Demo: Try It Yourself

<VSPlayground client:visible />

## Why It Works: Three-Panel Story

### 1. Hidden Bias (Intuition)

Human annotators systematically prefer familiar text - the "apple effect" where typical completions are rated higher regardless of true quality. This creates typicality bias (Œµ>0) in preference data.

### 2. Mathematical Collapse

The bias compounds during RLHF optimization. When many responses have equal utility (common in creative tasks), typicality becomes the tiebreaker, causing the model to collapse toward the mode:

$$\pi^*(y | x) \propto \pi_{ref}(y | x)^\rho \exp\left(\frac{r_{true}(x, y)}{\beta}\right)$$

### 3. VS Recovery

By asking for a distribution with probabilities, VS changes what constitutes the "mode". Instead of outputting the single most typical response, the model outputs a distribution approximating what it learned during pretraining.

<Figure id="states" caption="VS recovers the pretraining distribution. When asked to generate US states with probabilities, VS output closely matches the reference distribution (KL‚âà0.12), while direct prompting collapses to repeatedly outputting California and Texas.">
  <img src="/figures/states-distribution.png" alt="Distribution comparison" />
</Figure>

## Evidence: The Numbers Speak

<div className="grid gap-4 md:grid-cols-2 mb-8">
  <div className="evidence-card">
    <h4>üìä Diversity Gains</h4>
    <p className="claim">+1.6‚Äì2.1√ó increase</p>
    <p>Across poems, stories, and jokes</p>
    <p className="source">Figure 3a‚Äìc, pp. 7‚Äì8</p>
  </div>

  <div className="evidence-card">
    <h4>üë• Human Preference</h4>
    <p className="claim">+25.7% improvement</p>
    <p>Users prefer VS outputs</p>
    <p className="source">Table 3, p. 8</p>
  </div>

  <div className="evidence-card">
    <h4>üîÑ Retained Diversity</h4>
    <p className="claim">66.8% preserved</p>
    <p>Of base model creativity</p>
    <p className="source">Figure 4, p. 9</p>
  </div>

  <div className="evidence-card">
    <h4>üìà Scaling Benefit</h4>
    <p className="claim">~1.5‚Äì2√ó more gains</p>
    <p>Larger models benefit more</p>
    <p className="source">Figure 3e‚Äìf, p. 7</p>
  </div>
</div>

<!-- Scaling trend - one of the most compelling findings -->
<ScalingTrendVisualization client:visible />

<!-- VS beats fine-tuning callout -->
<VSBeatsFineTuningCallout client:load />

<!-- Synthetic data performance -->
<SyntheticDataPerformance client:visible />

## See the Difference: Real Examples

<QualitativeExamples client:load />

## How to Use VS: Variants & Recipes

<!-- Comprehensive comparison of VS variants -->
<VSVariantsComparison client:load className="mb-8" />

## Recipe Cards

<div className="grid gap-6 md:grid-cols-2 mb-8">
  <PromptCard
    client:load
    title="VS-Standard"
    description="Basic verbalized sampling - the simplest way to get started"
    prompt={`Generate k=5 {TASK} with their probabilities.
Return JSON: {"candidates":[{"text":"...", "prob":0.28}, ...]}
Only include candidates with probability ‚â• {œÑ}.
Ensure probabilities sum to 1.`}
    variant="standard"
    parameters={{ k: 5, threshold: 0.1 }}
  />

  <PromptCard
    client:load
    title="VS-CoT"
    description="Enhanced with chain-of-thought for better diversity"
    prompt={`Think step-by-step to enumerate distinct styles/approaches.
Then generate k=5 {TASK} with probabilities in JSON.
Only include items with probability ‚â• {œÑ}.`}
    variant="cot"
    parameters={{ k: 5, threshold: 0.15 }}
  />
</div>

## Is VS Right for You?

‚úÖ **Use VS when:**
- Generating creative content (stories, jokes, poems)
- Simulating diverse behaviors or dialogues
- Creating synthetic training data
- Exploring multiple valid solutions

‚ùå **Skip VS when:**
- You need a single, factual answer
- Task has one objectively correct response
- Speed is critical (VS uses more tokens)

<!-- Comprehensive FAQ section -->
<FAQ client:load />

## Conclusion

Mode collapse isn't just an algorithmic problem - it's baked into the preference data through typicality bias. Verbalized Sampling offers a simple, training-free solution: instead of asking for one response, ask for a distribution. The model already knows how to be diverse; we just need to ask it the right way.

The implications extend beyond creativity. VS improves dialogue simulation, open-ended QA, and synthetic data generation. Most importantly, it shows that aligned models retain their inherent diversity - we just need better ways to access it.

### Key Takeaways

1. **It's a data problem**: Typicality bias in preference data drives mode collapse
2. **VS is training-free**: Works immediately with any LLM - GPT-4, Claude, Gemini, or Llama
3. **Larger models benefit more**: 1.5-2√ó greater diversity gains with more capable models
4. **Real-world impact**: Improves synthetic data quality, leading to better downstream performance
5. **The model knows how to be diverse**: We just need to ask it the right way

## References