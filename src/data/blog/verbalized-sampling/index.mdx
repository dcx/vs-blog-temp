---
title: "Breaking Mode Collapse: How Verbalized Sampling Restores LLM Creativity"
description: "Ask for a distribution, not a single answer."
pubDatetime: 2025-10-03T10:00:00Z
tags: ["AI", "LLM", "Research", "Diversity"]
author: "VS Research Team"
featured: true
---

import VSPlayground from '@/components/interactives/VSPlayground.tsx';
import { Figure, Table, Equation, Sidenote } from '@/components/academic';

# Breaking Mode Collapse: How Verbalized Sampling Restores LLM Creativity

**Ask for a distribution, not a single answer.**

<div className="hero">
  <div>
    <h3>Direct Prompting</h3>
    <pre><code>"Tell me a joke about coffee"</code></pre>
    <p className="text-sm mt-2">Result: Same joke 5 times</p>
  </div>
  <div>
    <h3>Verbalized Sampling</h3>
    <pre><code>"Generate 5 jokes about coffee
with their probabilities"</code></pre>
    <p className="text-sm mt-2">Result: 5 different jokes</p>
  </div>
</div>

## TL;DR

- **Problem**: Post-training alignment causes mode collapse<Sidenote number={1}>Mode collapse reduces output diversity, making models favor stereotypical responses.</Sidenote>, reducing LLM creativity by 60-80% [@zhang2025vs]
- **Cause**: We identify *typicality bias* (Œµ>0) in preference data as a fundamental driver
- **Solution**: Verbalized Sampling (VS) - prompt for a distribution with probabilities to recover diversity

## The Problem Every AI User Knows

You ask for five jokes ‚Üí you get five of the **same** joke.
You ask for story ideas ‚Üí you get predictable plots.
Alignment sharpened the distribution; diversity collapsed.

The mathematical intuition is captured in Equation 3 from our paper [@zhang2025vs]:

$$\rho = 1 + \frac{\varepsilon}{\beta} > 1$$

Where typicality bias (Œµ>0) causes sharpening (œÅ>1), leading to mode collapse.

## Interactive Demo: Try It Yourself

<VSPlayground client:visible />

## Why It Works: Three-Panel Story

### 1. Hidden Bias (Intuition)

Human annotators systematically prefer familiar text - the "apple effect" where typical completions are rated higher regardless of true quality. This creates typicality bias (Œµ>0) in preference data.

### 2. Mathematical Collapse

The bias compounds during RLHF optimization. When many responses have equal utility (common in creative tasks), typicality becomes the tiebreaker, causing the model to collapse toward the mode:

$$\pi^*(y | x) \propto \pi_{ref}(y | x)^\rho \exp\left(\frac{r_{true}(x, y)}{\beta}\right)$$

### 3. VS Recovery

By asking for a distribution with probabilities, VS changes what constitutes the "mode". Instead of outputting the single most typical response, the model outputs a distribution approximating what it learned during pretraining.

<Figure id="states" caption="VS recovers the pretraining distribution. When asked to generate US states with probabilities, VS output closely matches the reference distribution (KL‚âà0.12), while direct prompting collapses to repeatedly outputting California and Texas.">
  <img src="/figures/states-distribution.png" alt="Distribution comparison" />
</Figure>

## Evidence: The Numbers Speak

<div className="grid gap-4 md:grid-cols-2">
  <div className="evidence-card">
    <h4>üìä Diversity Gains</h4>
    <p className="claim">+1.6‚Äì2.1√ó increase</p>
    <p>Across poems, stories, and jokes</p>
    <p className="source">Figure 3a‚Äìc, pp. 7‚Äì8</p>
  </div>

  <div className="evidence-card">
    <h4>üë• Human Preference</h4>
    <p className="claim">+25.7% improvement</p>
    <p>Users prefer VS outputs</p>
    <p className="source">Table 3, p. 8</p>
  </div>

  <div className="evidence-card">
    <h4>üîÑ Retained Diversity</h4>
    <p className="claim">66.8% preserved</p>
    <p>Of base model creativity</p>
    <p className="source">Figure 4, p. 9</p>
  </div>

  <div className="evidence-card">
    <h4>üìà Scaling Benefit</h4>
    <p className="claim">~1.5‚Äì2√ó more gains</p>
    <p>Larger models benefit more</p>
    <p className="source">Figure 3e‚Äìf, p. 7</p>
  </div>
</div>

## Copy-Ready Prompts

### VS-Standard (JSON Format)
```text
Generate k=5 {TASK} with their probabilities.
Return JSON: {"candidates":[{"text":"...", "prob":0.28}, ...]}
Only include candidates with probability ‚â• {œÑ}.
Ensure probabilities sum to 1.
```

### VS-CoT (Chain-of-Thought)
```text
Think step-by-step to enumerate distinct styles/approaches.
Then generate k=5 {TASK} with probabilities in JSON.
Only include items with probability ‚â• {œÑ}.
```

## Is VS Right for You?

‚úÖ **Use VS when:**
- Generating creative content (stories, jokes, poems)
- Simulating diverse behaviors or dialogues
- Creating synthetic training data
- Exploring multiple valid solutions

‚ùå **Skip VS when:**
- You need a single, factual answer
- Task has one objectively correct response
- Speed is critical (VS uses more tokens)

## Conclusion

Mode collapse isn't just an algorithmic problem - it's baked into the preference data through typicality bias. Verbalized Sampling offers a simple, training-free solution: instead of asking for one response, ask for a distribution. The model already knows how to be diverse; we just need to ask it the right way.

The implications extend beyond creativity. VS improves dialogue simulation, open-ended QA, and synthetic data generation. Most importantly, it shows that aligned models retain their inherent diversity - we just need better ways to access it.

## References